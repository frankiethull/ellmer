% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/provider-llamacpp.R
\name{chat_llamacpp}
\alias{chat_llamacpp}
\alias{models_llamacpp}
\title{Chat with a local llama.cpp server}
\usage{
chat_llamacpp(
  system_prompt = NULL,
  base_url = Sys.getenv("LLAMACPP_BASE_URL", "http://localhost:8080"),
  model = NULL,
  params = NULL,
  api_args = list(),
  echo = NULL,
  api_key = NULL,
  credentials = NULL,
  api_headers = character()
)

models_llamacpp(
  base_url = Sys.getenv("LLAMACPP_BASE_URL", "http://localhost:8080"),
  credentials = NULL
)
}
\arguments{
\item{system_prompt}{A system prompt to set the behavior of the assistant.}

\item{base_url}{The base URL for the llama.cpp server.}

\item{model}{The model name. If \code{NULL} (default), automatically uses
whichever model is currently loaded by the llama.cpp server.}

\item{params}{Common model parameters, usually created by \code{\link[=params]{params()}}.}

\item{api_args}{Named list of arbitrary extra arguments appended to the body
of every chat API call. Combined with the body object generated by ellmer
with \code{\link[=modifyList]{modifyList()}}.}

\item{echo}{One of the following options:
\itemize{
\item \code{none}: don't emit any output (default when running in a function).
\item \code{output}: echo text and tool-calling output as it streams in (default
when running at the console).
\item \code{all}: echo all input and output.
}

Note this only affects the \code{chat()} method.}

\item{api_key}{\ifelse{html}{\href{https://lifecycle.r-lib.org/articles/stages.html#deprecated}{\figure{lifecycle-deprecated.svg}{options: alt='[Deprecated]'}}}{\strong{[Deprecated]}} Use \code{credentials} instead.}

\item{credentials}{Optional credentials for authenticated access.}

\item{api_headers}{Named character vector of arbitrary extra headers appended
to every chat API call.}
}
\value{
A data frame with columns \code{id}, \code{created_at}, and \code{owned_by}.
}
\description{
To use \code{chat_llamacpp()} first download and install
\href{https://github.com/ggerganov/llama.cpp}{llama.cpp}. Then start the server
with a GGUF model file:

\if{html}{\out{<div class="sourceCode bash">}}\preformatted{./llama-server -m /path/to/model.gguf
}\if{html}{\out{</div>}}

llama.cpp can load GGUF files directly, including models from your local
Ollama installation (typically in \verb{~/.ollama/models/blobs/}), allowing you
to reuse already-downloaded models without duplicating storage.

Built on top of \code{\link[=chat_openai_compatible]{chat_openai_compatible()}}.
\subsection{Known limitations}{
\itemize{
\item Tool calling support depends on the specific model's capabilities
\item Only one model can be loaded at a time (unless using multiple server instances)
}
}

Queries the llama.cpp server to see which models are currently loaded.
Unlike Ollama, llama.cpp typically loads a single model at server startup.
}
\examples{
\dontrun{
# Start llama.cpp server first:
# ./llama-server -m /path/to/model.gguf

# Auto-detect loaded model
chat <- chat_llamacpp()
chat$chat("Tell me three jokes about statisticians")

# Or specify model name explicitly
chat <- chat_llamacpp(model = "llama-3.2-3b")

# Use an Ollama model with llama.cpp
# (find models with ollama_model_paths())
# ./llama-server -m ~/.ollama/models/blobs/sha256-abc123...
}
}
\seealso{
Other chatbots: 
\code{\link{chat_anthropic}()},
\code{\link{chat_aws_bedrock}()},
\code{\link{chat_azure_openai}()},
\code{\link{chat_cloudflare}()},
\code{\link{chat_databricks}()},
\code{\link{chat_deepseek}()},
\code{\link{chat_github}()},
\code{\link{chat_google_gemini}()},
\code{\link{chat_groq}()},
\code{\link{chat_huggingface}()},
\code{\link{chat_mistral}()},
\code{\link{chat_ollama}()},
\code{\link{chat_openai}()},
\code{\link{chat_openai_compatible}()},
\code{\link{chat_openrouter}()},
\code{\link{chat_perplexity}()},
\code{\link{chat_portkey}()}
}
\concept{chatbots}
